#!/usr/bin/env python3

import numpy as np
import torch
import time

from multi_drone_env import MultiDroneEnvironment
from algorithms.ham_dtan_maddpg import HAMDTANMADDPGAlgorithm


def test_core_algorithms():
    """æµ‹è¯•æ ¸å¿ƒç®—æ³•åŠŸèƒ½"""
    print("=" * 60)
    print("å¤šæ— äººæœºååŒå¼ºåŒ–å­¦ä¹ ç®—æ³•æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # 1. ç¯å¢ƒæµ‹è¯•
    print("1. æµ‹è¯•ç¯å¢ƒ...")
    env = MultiDroneEnvironment(num_friendly_drones=9, num_enemy_drones=3)
    obs, info = env.reset()
    
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  - è§‚æµ‹ç»´åº¦: {obs.shape}")
    print(f"  - æˆ‘æ–¹æ— äººæœº: {env.num_friendly_drones}æ¶")
    print(f"  - æ•Œæ–¹æ— äººæœº: {env.num_enemy_drones}æ¶")
    print(f"  - çŠ¶æ€ç©ºé—´: {env.map_size}")
    
    # 2. ç®—æ³•å®ä¾‹åŒ–æµ‹è¯•
    print("\n2. æµ‹è¯•HAM-DTAN-MADDPGç®—æ³•...")
    algorithm = HAMDTANMADDPGAlgorithm(
        num_agents=9,
        obs_dim=78,
        action_dim=3,
        num_enemies=3
    )
    
    print(f"âœ“ ç®—æ³•å®ä¾‹åŒ–æˆåŠŸ")
    print(f"  - æ€»å‚æ•°é‡: {algorithm.get_algorithm_info()['total_parameters']:,}")
    
    # 3. åŠ¨ä½œé€‰æ‹©æµ‹è¯•
    print("\n3. æµ‹è¯•åŠ¨ä½œé€‰æ‹©...")
    start_time = time.time()
    
    actions = algorithm.select_actions(obs, add_noise=False)
    action_time = time.time() - start_time
    
    print(f"âœ“ åŠ¨ä½œé€‰æ‹©æˆåŠŸ")
    print(f"  - åŠ¨ä½œç»´åº¦: {actions.shape}")
    print(f"  - é€‰æ‹©æ—¶é—´: {action_time*1000:.2f}ms")
    
    # 4. è¯¦ç»†ä¿¡æ¯è·å–æµ‹è¯•
    print("\n4. æµ‹è¯•æ³¨æ„åŠ›å’Œä»»åŠ¡åˆ†é…...")
    start_time = time.time()
    
    actions, infos = algorithm.select_actions(obs, add_noise=False, return_info=True)
    info_time = time.time() - start_time
    
    print(f"âœ“ è¯¦ç»†ä¿¡æ¯è·å–æˆåŠŸ")
    print(f"  - ä¿¡æ¯è·å–æ—¶é—´: {info_time*1000:.2f}ms")
    print(f"  - è¿”å›ä¿¡æ¯æ•°é‡: {len(infos)}")
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“çš„ä¿¡æ¯
    if infos and len(infos) > 0:
        agent_info = infos[0]
        if 'attention_info' in agent_info:
            att_info = agent_info['attention_info']
            print(f"  - ä¸»è¦ä»»åŠ¡: {att_info.get('primary_tasks', 'N/A')}")
            
        if 'dtan_results' in agent_info:
            dtan_info = agent_info['dtan_results']
            print(f"  - å½“å‰éœ€æ±‚: {dtan_info.get('current_demands', 'N/A')}")
            print(f"  - åˆ†é…çŸ©é˜µå½¢çŠ¶: {dtan_info.get('allocation_matrix', torch.tensor([])).shape}")
    
    # 5. ç¯å¢ƒäº¤äº’æµ‹è¯•
    print("\n5. æµ‹è¯•ç¯å¢ƒäº¤äº’...")
    total_reward = 0
    steps = 0
    
    for step in range(10):
        actions = algorithm.select_actions(obs, add_noise=True)
        next_obs, reward, terminated, truncated, step_info = env.step(actions)
        
        total_reward += reward
        steps += 1
        obs = next_obs
        
        if terminated or truncated:
            break
    
    print(f"âœ“ ç¯å¢ƒäº¤äº’æµ‹è¯•å®Œæˆ")
    print(f"  - äº¤äº’æ­¥æ•°: {steps}")
    print(f"  - ç´¯è®¡å¥–åŠ±: {total_reward:.2f}")
    print(f"  - å¹³å‡å¥–åŠ±: {total_reward/steps:.2f}")
    
    # 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\n6. æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # æµ‹è¯•é€‰æ‹©åŠ¨ä½œçš„æ€§èƒ½
    obs_batch = [obs for _ in range(100)]
    start_time = time.time()
    
    for test_obs in obs_batch:
        _ = algorithm.select_actions(test_obs, add_noise=False)
    
    batch_time = time.time() - start_time
    avg_time = batch_time / 100
    
    print(f"âœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print(f"  - 100æ¬¡åŠ¨ä½œé€‰æ‹©æ€»æ—¶é—´: {batch_time:.3f}s")
    print(f"  - å¹³å‡å•æ¬¡æ—¶é—´: {avg_time*1000:.2f}ms")
    print(f"  - ç†è®ºFPS: {1/avg_time:.1f}")
    
    # 7. ç®—æ³•ç»„ä»¶æµ‹è¯•
    print("\n7. æµ‹è¯•ç®—æ³•ç»„ä»¶...")
    
    # æµ‹è¯•HAM
    print("  æµ‹è¯•å±‚æ¬¡åŒ–æ³¨æ„åŠ›æœºåˆ¶(HAM)...")
    ham_module = algorithm.agents[0].actor.ham
    ham_features, ham_info = ham_module(torch.FloatTensor(obs).unsqueeze(0))
    print(f"    âœ“ HAMè¾“å‡ºç»´åº¦: {ham_features.shape}")
    print(f"    âœ“ ç©ºé—´æ³¨æ„åŠ›æƒé‡: {ham_info['spatial_weights'].shape if ham_info['spatial_weights'] is not None else 'None'}")
    
    # æµ‹è¯•DTAN
    print("  æµ‹è¯•åŠ¨æ€ä»»åŠ¡åˆ†é…ç½‘ç»œ(DTAN)...")
    dtan_module = algorithm.agents[0].actor.dtan
    dtan_results = dtan_module(torch.FloatTensor(obs).unsqueeze(0))
    print(f"    âœ“ DTANåˆ†é…çŸ©é˜µ: {dtan_results['allocation_matrix'].shape}")
    print(f"    âœ“ å½“å‰éœ€æ±‚: {dtan_results['current_demands'].shape}")
    print(f"    âœ“ é¢„æµ‹éœ€æ±‚: {dtan_results['predicted_demands'].shape}")
    
    # 8. å†…å­˜ä½¿ç”¨æµ‹è¯•
    print("\n8. å†…å­˜ä½¿ç”¨æµ‹è¯•...")
    
    # é‡ç½®ç®—æ³•å†…å­˜
    algorithm.reset_memory()
    print(f"  âœ“ å†…å­˜é‡ç½®å®Œæˆ")
    
    # å™ªå£°é‡ç½®
    algorithm.reset_noise()
    print(f"  âœ“ å™ªå£°é‡ç½®å®Œæˆ")
    
    env.close()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    
    return True


def test_training_components():
    """æµ‹è¯•è®­ç»ƒç›¸å…³ç»„ä»¶"""
    print("\n" + "=" * 60)
    print("è®­ç»ƒç»„ä»¶æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç®—æ³•å®ä¾‹
    algorithm = HAMDTANMADDPGAlgorithm()
    env = MultiDroneEnvironment()
    
    # 1. ç»éªŒå­˜å‚¨æµ‹è¯•
    print("1. æµ‹è¯•ç»éªŒå­˜å‚¨...")
    
    obs, _ = env.reset()
    actions = algorithm.select_actions(obs)
    next_obs, reward, terminated, truncated, info = env.step(actions)
    
    # å­˜å‚¨ç»éªŒ
    algorithm.store_transition(obs, actions.flatten(), reward, next_obs, terminated or truncated)
    
    print(f"  âœ“ ç»éªŒå­˜å‚¨æˆåŠŸ")
    print(f"  âœ“ ç»éªŒæ± å¤§å°: {len(algorithm.replay_buffer)}")
    
    # 2. æ‰¹é‡ç»éªŒç”Ÿæˆ
    print("\n2. ç”Ÿæˆæ‰¹é‡ç»éªŒ...")
    
    for _ in range(100):
        obs, _ = env.reset()
        algorithm.reset_noise()
        
        for step in range(10):
            actions = algorithm.select_actions(obs, add_noise=True)
            next_obs, reward, terminated, truncated, info = env.step(actions)
            
            algorithm.store_transition(obs, actions.flatten(), reward, next_obs, terminated or truncated)
            
            obs = next_obs
            if terminated or truncated:
                break
    
    print(f"  âœ“ æ‰¹é‡ç»éªŒç”Ÿæˆå®Œæˆ")
    print(f"  âœ“ æœ€ç»ˆç»éªŒæ± å¤§å°: {len(algorithm.replay_buffer)}")
    
    # 3. ç½‘ç»œæ›´æ–°æµ‹è¯•
    print("\n3. æµ‹è¯•ç½‘ç»œæ›´æ–°...")
    
    if len(algorithm.replay_buffer) >= 256:
        start_time = time.time()
        losses = algorithm.update(batch_size=256)
        update_time = time.time() - start_time
        
        print(f"  âœ“ ç½‘ç»œæ›´æ–°æˆåŠŸ")
        print(f"  âœ“ æ›´æ–°æ—¶é—´: {update_time:.3f}s")
        print(f"  âœ“ æŸå¤±ä¿¡æ¯æ•°é‡: {len(losses)}")
        
        # æ‰“å°éƒ¨åˆ†æŸå¤±ä¿¡æ¯
        sample_losses = list(losses.items())[:3]
        for key, value in sample_losses:
            print(f"    - {key}: {value:.4f}")
    else:
        print(f"  âš  ç»éªŒä¸è¶³ï¼Œè·³è¿‡æ›´æ–°æµ‹è¯•")
    
    env.close()
    print("\nâœ… è®­ç»ƒç»„ä»¶æµ‹è¯•å®Œæˆ")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹HAM-DTAN-MADDPGç®—æ³•æµ‹è¯•...")
    
    try:
        # æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
        success = test_core_algorithms()
        
        if success:
            # è®­ç»ƒç»„ä»¶æµ‹è¯•
            test_training_components()
            
            print("\nğŸŠ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼ç®—æ³•å·²å‡†å¤‡å°±ç»ªã€‚")
            print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
            print("  1. è¿è¡Œ python experiments/training_script.py å¼€å§‹è®­ç»ƒ")
            print("  2. è¿è¡Œ python experiments/visualization.py ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
            print("  3. è¿è¡Œ python demo_visualization.py æŸ¥çœ‹å®æ—¶æ¼”ç¤º")
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    main()