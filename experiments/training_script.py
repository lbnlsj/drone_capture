#!/usr/bin/env python3

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import time
import json
from datetime import datetime

from multi_drone_env import MultiDroneEnvironment
from algorithms.ham_dtan_maddpg import HAMDTANMADDPGAlgorithm


class ExperimentTracker:
    """å®éªŒè·Ÿè¸ªå™¨"""
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.start_time = datetime.now()
        self.metrics = {
            'episodes': [],
            'rewards': [],
            'success_rates': [],
            'coverage_rates': [],
            'intercept_rates': [],
            'episode_lengths': [],
            'training_losses': []
        }
        
    def log_episode(self, episode, total_reward, success, coverage_rate, intercept_rate, episode_length):
        """è®°å½•å•ä¸ªepisodeçš„ç»“æœ"""
        self.metrics['episodes'].append(episode)
        self.metrics['rewards'].append(total_reward)
        self.metrics['success_rates'].append(1.0 if success else 0.0)
        self.metrics['coverage_rates'].append(coverage_rate)
        self.metrics['intercept_rates'].append(intercept_rate)
        self.metrics['episode_lengths'].append(episode_length)
    
    def log_training_loss(self, losses):
        """è®°å½•è®­ç»ƒæŸå¤±"""
        self.metrics['training_losses'].append(losses)
    
    def get_recent_performance(self, window=100):
        """è·å–æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡"""
        if len(self.metrics['rewards']) < window:
            window = len(self.metrics['rewards'])
        
        if window == 0:
            return {}
        
        recent_rewards = self.metrics['rewards'][-window:]
        recent_success = self.metrics['success_rates'][-window:]
        recent_coverage = self.metrics['coverage_rates'][-window:]
        recent_intercept = self.metrics['intercept_rates'][-window:]
        
        return {
            'avg_reward': np.mean(recent_rewards),
            'success_rate': np.mean(recent_success),
            'coverage_rate': np.mean(recent_coverage),
            'intercept_rate': np.mean(recent_intercept),
            'episodes': window
        }
    
    def save_results(self, filepath):
        """ä¿å­˜å®éªŒç»“æœ"""
        results = {
            'experiment_name': self.experiment_name,
            'start_time': self.start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'total_episodes': len(self.metrics['episodes']),
            'metrics': self.metrics,
            'final_performance': self.get_recent_performance()
        }
        
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=2)


def evaluate_algorithm(algorithm, env, num_episodes=100, deterministic=True):
    """è¯„ä¼°ç®—æ³•æ€§èƒ½"""
    total_rewards = []
    success_rates = []
    coverage_rates = []
    intercept_rates = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        total_reward = 0
        episode_length = 0
        
        # é‡ç½®ç®—æ³•çŠ¶æ€
        if hasattr(algorithm, 'reset_noise'):
            algorithm.reset_noise()
        if hasattr(algorithm, 'reset_memory'):
            algorithm.reset_memory()
        
        done = False
        while not done and episode_length < 500:
            # é€‰æ‹©åŠ¨ä½œ
            actions = algorithm.select_actions(obs, add_noise=not deterministic)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, terminated, truncated, info = env.step(actions)
            
            total_reward += reward
            episode_length += 1
            done = terminated or truncated
            obs = next_obs
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        final_coverage = sum(status['satisfied'] for status in info['coverage_status'])
        final_intercept = sum(status['intercepted'] for status in info['intercept_status'])
        
        coverage_rate = final_coverage / len(info['coverage_status'])
        intercept_rate = final_intercept / len(info['intercept_status'])
        success = terminated  # ä»»åŠ¡æˆåŠŸå®Œæˆ
        
        total_rewards.append(total_reward)
        success_rates.append(1.0 if success else 0.0)
        coverage_rates.append(coverage_rate)
        intercept_rates.append(intercept_rate)
        episode_lengths.append(episode_length)
    
    return {
        'avg_reward': np.mean(total_rewards),
        'std_reward': np.std(total_rewards),
        'success_rate': np.mean(success_rates),
        'coverage_rate': np.mean(coverage_rates),
        'intercept_rate': np.mean(intercept_rates),
        'avg_episode_length': np.mean(episode_lengths),
        'total_episodes': num_episodes
    }




def train_ham_dtan_maddpg(env, num_episodes=2000):
    """è®­ç»ƒHAM-DTAN-MADDPG"""
    print("å¼€å§‹è®­ç»ƒHAM-DTAN-MADDPG...")
    
    obs_dim = env.observation_space.shape[0]
    action_dim = 3
    num_agents = env.num_friendly_drones
    num_enemies = env.num_enemy_drones
    
    algorithm = HAMDTANMADDPGAlgorithm(
        num_agents=num_agents,
        obs_dim=obs_dim,
        action_dim=action_dim,
        num_enemies=num_enemies
    )
    
    tracker = ExperimentTracker("HAM_DTAN_MADDPG")
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        algorithm.reset_noise()
        algorithm.reset_memory()
        
        total_reward = 0
        episode_length = 0
        
        done = False
        while not done and episode_length < 500:
            # é€‰æ‹©åŠ¨ä½œ
            actions = algorithm.select_actions(obs, add_noise=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, terminated, truncated, next_info = env.step(actions)
            
            # å­˜å‚¨ç»éªŒ
            algorithm.store_transition(obs, actions.flatten(), reward, next_obs, terminated or truncated)
            
            total_reward += reward
            episode_length += 1
            done = terminated or truncated
            obs = next_obs
            info = next_info
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        final_coverage = sum(status['satisfied'] for status in info['coverage_status'])
        final_intercept = sum(status['intercepted'] for status in info['intercept_status'])
        
        coverage_rate = final_coverage / len(info['coverage_status'])
        intercept_rate = final_intercept / len(info['intercept_status'])
        success = terminated
        
        # è®°å½•ç»“æœ
        tracker.log_episode(episode, total_reward, success, coverage_rate, intercept_rate, episode_length)
        
        # æ›´æ–°ç½‘ç»œ
        if episode % 2 == 0 and len(algorithm.replay_buffer) > 1000:
            losses = algorithm.update(batch_size=256)
            if losses:
                tracker.log_training_loss(losses)
        
        # æ‰“å°è¿›åº¦
        if episode % 100 == 0:
            recent_perf = tracker.get_recent_performance(100)
            print(f"Episode {episode}: "
                  f"Avg Reward: {recent_perf.get('avg_reward', 0):.2f}, "
                  f"Success Rate: {recent_perf.get('success_rate', 0):.2f}, "
                  f"Coverage Rate: {recent_perf.get('coverage_rate', 0):.2f}")
            
            # æ‰“å°ç®—æ³•ä¿¡æ¯
            if episode % 500 == 0:
                print(f"ç®—æ³•ä¿¡æ¯: {algorithm.get_algorithm_info()}")
    
    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    algorithm.save('models/ham_dtan_maddpg.pth')
    tracker.save_results('results/ham_dtan_maddpg_results.json')
    
    return algorithm, tracker


def run_ham_dtan_experiment():
    """è¿è¡ŒHAM-DTAN-MADDPGå®éªŒ"""
    print("=" * 80)
    print("HAM-DTAN-MADDPG å¤šæ— äººæœºååŒå¼ºåŒ–å­¦ä¹ å®éªŒ")
    print("=" * 80)
    
    # åˆ›å»ºç›®å½•
    os.makedirs('models', exist_ok=True)
    os.makedirs('results', exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒ
    env = MultiDroneEnvironment(num_friendly_drones=9, num_enemy_drones=3)
    
    print(f"ç¯å¢ƒä¿¡æ¯:")
    print(f"- è§‚æµ‹ç©ºé—´: {env.observation_space.shape}")
    print(f"- åŠ¨ä½œç©ºé—´: {env.action_space.shape}")
    print(f"- æˆ‘æ–¹æ— äººæœº: {env.num_friendly_drones}")
    print(f"- æ•Œæ–¹æ— äººæœº: {env.num_enemy_drones}")
    print(f"- 3Dç©ºé—´: {env.map_size}")
    print()
    
    # è®­ç»ƒHAM-DTAN-MADDPG
    print("å¼€å§‹è®­ç»ƒHAM-DTAN-MADDPGç®—æ³•...")
    start_time = time.time()
    ham_dtan_agent, ham_dtan_tracker = train_ham_dtan_maddpg(env, num_episodes=2000)
    training_time = time.time() - start_time
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæ€§èƒ½: {ham_dtan_tracker.get_recent_performance()}")
    
    # è¯„ä¼°ç®—æ³•
    print("\nğŸ” å¼€å§‹æœ€ç»ˆè¯„ä¼°...")
    eval_results = evaluate_algorithm(ham_dtan_agent, env, num_episodes=200, deterministic=True)
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ¯ HAM-DTAN-MADDPG å®éªŒç»“æœ")
    print("=" * 80)
    
    print(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"  - å¹³å‡å¥–åŠ±: {eval_results['avg_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
    print(f"  - ä»»åŠ¡æˆåŠŸç‡: {eval_results['success_rate']:.2%}")
    print(f"  - ç›®æ ‡è¦†ç›–ç‡: {eval_results['coverage_rate']:.2%}")
    print(f"  - æ‹¦æˆªæˆåŠŸç‡: {eval_results['intercept_rate']:.2%}")
    print(f"  - å¹³å‡episodeé•¿åº¦: {eval_results['avg_episode_length']:.1f}æ­¥")
    
    print(f"\nâ±ï¸ è®­ç»ƒç»Ÿè®¡:")
    print(f"  - æ€»è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
    print(f"  - å¹³å‡æ¯episode: {training_time/2000:.3f}ç§’")
    print(f"  - ç®—æ³•å‚æ•°é‡: {ham_dtan_agent.get_algorithm_info()['total_parameters']:,}")
    
    # ä¿å­˜ç»“æœ
    final_results = {
        'algorithm': 'HAM-DTAN-MADDPG',
        'experiment_time': datetime.now().isoformat(),
        'training_time': training_time,
        'training_episodes': 2000,
        'evaluation_episodes': 200,
        'final_performance': ham_dtan_tracker.get_recent_performance(),
        'evaluation_results': eval_results,
        'algorithm_info': ham_dtan_agent.get_algorithm_info(),
        'environment_config': {
            'num_friendly_drones': env.num_friendly_drones,
            'num_enemy_drones': env.num_enemy_drones,
            'map_size': env.map_size,
            'obs_dim': env.observation_space.shape[0],
            'action_dim': env.action_space.shape
        }
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('results/ham_dtan_experiment_results.json', 'w') as f:
        json.dump(final_results, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ° models/ ç›®å½•")
    
    env.close()
    
    return final_results


if __name__ == "__main__":
    try:
        results = run_ham_dtan_experiment()
        print("\nğŸ‰ HAM-DTAN-MADDPGå®éªŒå®Œæˆï¼")
        print("ğŸ“Š å¯ä»¥è¿è¡Œ python experiments/visualization.py æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")
        print("ğŸ® å¯ä»¥è¿è¡Œ python demo_visualization.py æŸ¥çœ‹å®æ—¶æ¼”ç¤º")
    except KeyboardInterrupt:
        print("\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()